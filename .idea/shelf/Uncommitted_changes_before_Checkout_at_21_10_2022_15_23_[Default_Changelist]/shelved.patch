Index: notebooks/.ipynb_checkpoints/Coursework_1-checkpoint.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Coursework 1\\n\",\r\n    \"\\n\",\r\n    \"This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import matplotlib.pyplot as plt\\n\",\r\n    \"%matplotlib inline\\n\",\r\n    \"plt.style.use('ggplot')\\n\",\r\n    \"\\n\",\r\n    \"def train_model_and_plot_stats(\\n\",\r\n    \"        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\\n\",\r\n    \"    \\n\",\r\n    \"    # As well as monitoring the error over training also monitor classification\\n\",\r\n    \"    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\\n\",\r\n    \"    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\\n\",\r\n    \"\\n\",\r\n    \"    # Use the created objects to initialise a new Optimiser instance.\\n\",\r\n    \"    optimiser = Optimiser(\\n\",\r\n    \"        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\\n\",\r\n    \"\\n\",\r\n    \"    # Run the optimiser for num_epochs epochs (full passes through the training set)\\n\",\r\n    \"    # printing statistics every epoch.\\n\",\r\n    \"    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\\n\",\r\n    \"\\n\",\r\n    \"    # Plot the change in the validation and training set error over training.\\n\",\r\n    \"    fig_1 = plt.figure(figsize=(8, 4))\\n\",\r\n    \"    ax_1 = fig_1.add_subplot(111)\\n\",\r\n    \"    for k in ['error(train)', 'error(valid)']:\\n\",\r\n    \"        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \\n\",\r\n    \"                  stats[1:, keys[k]], label=k)\\n\",\r\n    \"    ax_1.legend(loc=0)\\n\",\r\n    \"    ax_1.set_xlabel('Epoch number')\\n\",\r\n    \"    ax_1.set_ylabel('Error')\\n\",\r\n    \"\\n\",\r\n    \"    # Plot the change in the validation and training set accuracy over training.\\n\",\r\n    \"    fig_2 = plt.figure(figsize=(8, 4))\\n\",\r\n    \"    ax_2 = fig_2.add_subplot(111)\\n\",\r\n    \"    for k in ['acc(train)', 'acc(valid)']:\\n\",\r\n    \"        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \\n\",\r\n    \"                  stats[1:, keys[k]], label=k)\\n\",\r\n    \"    ax_2.legend(loc=0)\\n\",\r\n    \"    ax_2.set_xlabel('Epoch number')\\n\",\r\n    \"    ax_2.set_xlabel('Accuracy')\\n\",\r\n    \"    \\n\",\r\n    \"    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# The below code will set up the data providers, random number\\n\",\r\n    \"# generator and logger objects needed for training runs. As\\n\",\r\n    \"# loading the data from file take a little while you generally\\n\",\r\n    \"# will probably not want to reload the data providers on\\n\",\r\n    \"# every training run. If you wish to reset their state you\\n\",\r\n    \"# should instead use the .reset() method of the data providers.\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"import logging\\n\",\r\n    \"from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\\n\",\r\n    \"\\n\",\r\n    \"# Seed a random number generator\\n\",\r\n    \"seed = 11102019 \\n\",\r\n    \"rng = np.random.RandomState(seed)\\n\",\r\n    \"batch_size = 100\\n\",\r\n    \"# Set up a logger object to print info about the training run to stdout\\n\",\r\n    \"logger = logging.getLogger()\\n\",\r\n    \"logger.setLevel(logging.INFO)\\n\",\r\n    \"logger.handlers = [logging.StreamHandler()]\\n\",\r\n    \"\\n\",\r\n    \"# Create data provider objects for the MNIST data set\\n\",\r\n    \"train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\\n\",\r\n    \"valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# The model set up code below is provided as a starting point.\\n\",\r\n    \"# You will probably want to add further code cells for the\\n\",\r\n    \"# different experiments you run.\\n\",\r\n    \"\\n\",\r\n    \"from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer\\n\",\r\n    \"from mlp.errors import CrossEntropySoftmaxError\\n\",\r\n    \"from mlp.models import MultipleLayerModel\\n\",\r\n    \"from mlp.initialisers import ConstantInit, GlorotUniformInit\\n\",\r\n    \"from mlp.learning_rules import AdamLearningRule\\n\",\r\n    \"from mlp.optimisers import Optimiser\\n\",\r\n    \"\\n\",\r\n    \"# Setup hyperparameters\\n\",\r\n    \"learning_rate = 0.001\\n\",\r\n    \"num_epochs = 100\\n\",\r\n    \"stats_interval = 1\\n\",\r\n    \"input_dim, output_dim, hidden_dim = 784, 47, 128\\n\",\r\n    \"\\n\",\r\n    \"weights_init = GlorotUniformInit(rng=rng)\\n\",\r\n    \"biases_init = ConstantInit(0.)\\n\",\r\n    \"\\n\",\r\n    \"# Create model with ONE hidden layer\\n\",\r\n    \"model = MultipleLayerModel([\\n\",\r\n    \"    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\\n\",\r\n    \"])\\n\",\r\n    \"\\n\",\r\n    \"error = CrossEntropySoftmaxError()\\n\",\r\n    \"# Use a Adam learning rule\\n\",\r\n    \"learning_rule = AdamLearningRule(learning_rate=learning_rate)\\n\",\r\n    \"\\n\",\r\n    \"# Remember to use notebook=False when you write a script to be run in a terminal\\n\",\r\n    \"_ = train_model_and_plot_stats(\\n\",\r\n    \"    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# Create model with TWO hidden layers\\n\",\r\n    \"model = MultipleLayerModel([\\n\",\r\n    \"    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\\n\",\r\n    \"])\\n\",\r\n    \"\\n\",\r\n    \"error = CrossEntropySoftmaxError()\\n\",\r\n    \"# Use a Adam learning rule\\n\",\r\n    \"learning_rule = AdamLearningRule(learning_rate=learning_rate)\\n\",\r\n    \"\\n\",\r\n    \"# Remember to use notebook=False when you write a script to be run in a terminal\\n\",\r\n    \"_ = train_model_and_plot_stats(\\n\",\r\n    \"    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.9.7\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 1\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/.ipynb_checkpoints/Coursework_1-checkpoint.ipynb b/notebooks/.ipynb_checkpoints/Coursework_1-checkpoint.ipynb
--- a/notebooks/.ipynb_checkpoints/Coursework_1-checkpoint.ipynb	(revision f8f0f0736a6e611895d9f72d4355d9d2648427aa)
+++ b/notebooks/.ipynb_checkpoints/Coursework_1-checkpoint.ipynb	(date 1666361966922)
@@ -11,7 +11,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -59,9 +59,21 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 2,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "ename": "ModuleNotFoundError",
+     "evalue": "No module named 'mlp'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
+      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_providers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNISTDataProvider, EMNISTDataProvider\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Seed a random number generator\u001b[39;00m\n\u001b[0;32m     12\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11102019\u001b[39m \n",
+      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlp'"
+     ]
+    }
+   ],
    "source": [
     "# The below code will set up the data providers, random number\n",
     "# generator and logger objects needed for training runs. As\n",
@@ -69,6 +81,8 @@
     "# will probably not want to reload the data providers on\n",
     "# every training run. If you wish to reset their state you\n",
     "# should instead use the .reset() method of the data providers.\n",
+    "import sys\n",
+    "sys.path.append('C:/Users/admin/Desktop/mlpractical')\n",
     "import numpy as np\n",
     "import logging\n",
     "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
@@ -156,7 +170,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3",
+   "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
@@ -170,9 +184,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.9.7"
+   "version": "3.9.12"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 1
+ "nbformat_minor": 4
 }
Index: notebooks/DropoutandPenalty_tests.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {\r\n    \"scrolled\": true\r\n   },\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from mlp.test_methods import test_dropout_layer\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"\\n\",\r\n    \"fprop_test, fprop_output, fprop_correct, \\\\\\n\",\r\n    \"bprop_test, bprop_output, bprop_correct = test_dropout_layer()\\n\",\r\n    \"\\n\",\r\n    \"assert fprop_test == 1.0, (\\n\",\r\n    \"'The dropout layer fprop functionality test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(fprop_correct, fprop_output, fprop_output-fprop_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"Dropout Layer Fprop Functionality Test Passed\\\")\\n\",\r\n    \"\\n\",\r\n    \"assert bprop_test == 1.0, (\\n\",\r\n    \"'The dropout layer bprop functionality test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(bprop_correct, bprop_output, bprop_output-bprop_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"Dropout Layer Bprop Test Passed\\\")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from mlp.test_methods import test_L1_Penalty\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"call_test, call_output, call_correct, \\\\\\n\",\r\n    \"grad_test, grad_output, grad_correct = test_L1_Penalty()\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"assert call_test == 1.0, (\\n\",\r\n    \"'The call function for L1 Penalty test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(call_correct, call_output, call_output-call_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"L1 Penalty Call Functionality Test Passed\\\")\\n\",\r\n    \"\\n\",\r\n    \"assert grad_test == 1.0, (\\n\",\r\n    \"'The grad function for L1 Penalty test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(grad_correct, grad_output, grad_output-grad_correct, grad_output/grad_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"L1 Penalty Grad Function Test Passed\\\")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from mlp.test_methods import test_L2_Penalty\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"call_test, call_output, call_correct, \\\\\\n\",\r\n    \"grad_test, grad_output, grad_correct = test_L2_Penalty()\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"assert call_test == 1.0, (\\n\",\r\n    \"'The call function for L2 Penalty test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(call_correct, call_output, call_output-call_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"L2 Penalty Call Functionality Test Passed\\\")\\n\",\r\n    \"\\n\",\r\n    \"assert grad_test == 1.0, (\\n\",\r\n    \"'The grad function for L2 Penalty test failed'\\n\",\r\n    \"'Correct output is \\\\n\\\\n{0}\\\\n\\\\n but returned output is \\\\n\\\\n{1}\\\\n\\\\n difference is \\\\n\\\\n{2}'\\n\",\r\n    \".format(grad_correct, grad_output, grad_output-grad_correct, grad_output/grad_correct)\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"\\n\",\r\n    \"print(\\\"L2 Penalty Grad Function Test Passed\\\")\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.9.7\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 1\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/DropoutandPenalty_tests.ipynb b/notebooks/DropoutandPenalty_tests.ipynb
--- a/notebooks/DropoutandPenalty_tests.ipynb	(revision f8f0f0736a6e611895d9f72d4355d9d2648427aa)
+++ b/notebooks/DropoutandPenalty_tests.ipynb	(date 1666360934312)
@@ -2,15 +2,30 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 3,
    "metadata": {
     "scrolled": true
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "ename": "NotImplementedError",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
+      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_methods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_dropout_layer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m fprop_test, fprop_output, fprop_correct, \\\n\u001b[1;32m----> 6\u001b[0m bprop_test, bprop_output, bprop_correct \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dropout_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fprop_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, (\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dropout layer fprop functionality test failed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrect output is \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m but returned output is \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m difference is \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;241m.\u001b[39mformat(fprop_correct, fprop_output, fprop_output\u001b[38;5;241m-\u001b[39mfprop_correct)\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Layer Fprop Functionality Test Passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "File \u001b[1;32mC:\\Users/admin/Desktop/mlpractical\\mlp\\test_methods.py:21\u001b[0m, in \u001b[0;36mtest_dropout_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(\u001b[38;5;241m92019\u001b[39m)\n\u001b[0;32m     19\u001b[0m     layer \u001b[38;5;241m=\u001b[39m DropoutLayer(rng\u001b[38;5;241m=\u001b[39mrng)\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     grads \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbprop(inputs\u001b[38;5;241m=\u001b[39mx, outputs\u001b[38;5;241m=\u001b[39mout, grads_wrt_outputs\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones(x\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     correct_outputs = correct_outputs['dropout']\u001b[39;00m\n",
+      "File \u001b[1;32mC:\\Users/admin/Desktop/mlpractical\\mlp\\layers.py:664\u001b[0m, in \u001b[0;36mDropoutLayer.fprop\u001b[1;34m(self, inputs, stochastic)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfprop\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, stochastic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;124;03m\"\"\"Forward propagates activations through the layer transformation.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03m        outputs: Array of layer outputs of shape (batch_size, output_dim).\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
+      "\u001b[1;31mNotImplementedError\u001b[0m: "
+     ]
+    }
+   ],
    "source": [
+    "import sys\n",
+    "sys.path.append('C:/Users/admin/Desktop/mlpractical')\n",
     "from mlp.test_methods import test_dropout_layer\n",
     "import numpy as np\n",
-    "\n",
     "fprop_test, fprop_output, fprop_correct, \\\n",
     "bprop_test, bprop_output, bprop_correct = test_dropout_layer()\n",
     "\n",
@@ -98,7 +113,7 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "Python 3",
+   "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
@@ -112,7 +127,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.9.7"
+   "version": "3.9.12"
   }
  },
  "nbformat": 4,
Index: notebooks/Coursework_1.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Coursework 1\\n\",\r\n    \"\\n\",\r\n    \"This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 1,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"import matplotlib.pyplot as plt\\n\",\r\n    \"%matplotlib inline\\n\",\r\n    \"plt.style.use('ggplot')\\n\",\r\n    \"\\n\",\r\n    \"def train_model_and_plot_stats(\\n\",\r\n    \"        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\\n\",\r\n    \"    \\n\",\r\n    \"    # As well as monitoring the error over training also monitor classification\\n\",\r\n    \"    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\\n\",\r\n    \"    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\\n\",\r\n    \"\\n\",\r\n    \"    # Use the created objects to initialise a new Optimiser instance.\\n\",\r\n    \"    optimiser = Optimiser(\\n\",\r\n    \"        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\\n\",\r\n    \"\\n\",\r\n    \"    # Run the optimiser for num_epochs epochs (full passes through the training set)\\n\",\r\n    \"    # printing statistics every epoch.\\n\",\r\n    \"    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\\n\",\r\n    \"\\n\",\r\n    \"    # Plot the change in the validation and training set error over training.\\n\",\r\n    \"    fig_1 = plt.figure(figsize=(8, 4))\\n\",\r\n    \"    ax_1 = fig_1.add_subplot(111)\\n\",\r\n    \"    for k in ['error(train)', 'error(valid)']:\\n\",\r\n    \"        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \\n\",\r\n    \"                  stats[1:, keys[k]], label=k)\\n\",\r\n    \"    ax_1.legend(loc=0)\\n\",\r\n    \"    ax_1.set_xlabel('Epoch number')\\n\",\r\n    \"    ax_1.set_ylabel('Error')\\n\",\r\n    \"\\n\",\r\n    \"    # Plot the change in the validation and training set accuracy over training.\\n\",\r\n    \"    fig_2 = plt.figure(figsize=(8, 4))\\n\",\r\n    \"    ax_2 = fig_2.add_subplot(111)\\n\",\r\n    \"    for k in ['acc(train)', 'acc(valid)']:\\n\",\r\n    \"        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \\n\",\r\n    \"                  stats[1:, keys[k]], label=k)\\n\",\r\n    \"    ax_2.legend(loc=0)\\n\",\r\n    \"    ax_2.set_xlabel('Epoch number')\\n\",\r\n    \"    ax_2.set_xlabel('Accuracy')\\n\",\r\n    \"    \\n\",\r\n    \"    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 2,\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"KeysView(<numpy.lib.npyio.NpzFile object at 0x000002149A730280>)\\n\",\r\n      \"KeysView(<numpy.lib.npyio.NpzFile object at 0x000002149A7303A0>)\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"# The below code will set up the data providers, random number\\n\",\r\n    \"# generator and logger objects needed for training runs. As\\n\",\r\n    \"# loading the data from file take a little while you generally\\n\",\r\n    \"# will probably not want to reload the data providers on\\n\",\r\n    \"# every training run. If you wish to reset their state you\\n\",\r\n    \"# should instead use the .reset() method of the data providers.\\n\",\r\n    \"import numpy as np\\n\",\r\n    \"import logging\\n\",\r\n    \"from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\\n\",\r\n    \"\\n\",\r\n    \"# Seed a random number generator\\n\",\r\n    \"seed = 11102019 \\n\",\r\n    \"rng = np.random.RandomState(seed)\\n\",\r\n    \"batch_size = 100\\n\",\r\n    \"# Set up a logger object to print info about the training run to stdout\\n\",\r\n    \"logger = logging.getLogger()\\n\",\r\n    \"logger.setLevel(logging.INFO)\\n\",\r\n    \"logger.handlers = [logging.StreamHandler()]\\n\",\r\n    \"\\n\",\r\n    \"# Create data provider objects for the MNIST data set\\n\",\r\n    \"train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\\n\",\r\n    \"valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# The model set up code below is provided as a starting point.\\n\",\r\n    \"# You will probably want to add further code cells for the\\n\",\r\n    \"# different experiments you run.\\n\",\r\n    \"\\n\",\r\n    \"from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer\\n\",\r\n    \"from mlp.errors import CrossEntropySoftmaxError\\n\",\r\n    \"from mlp.models import MultipleLayerModel\\n\",\r\n    \"from mlp.initialisers import ConstantInit, GlorotUniformInit\\n\",\r\n    \"from mlp.learning_rules import AdamLearningRule\\n\",\r\n    \"from mlp.optimisers import Optimiser\\n\",\r\n    \"\\n\",\r\n    \"# Setup hyperparameters\\n\",\r\n    \"learning_rate = 0.001\\n\",\r\n    \"num_epochs = 100\\n\",\r\n    \"stats_interval = 1\\n\",\r\n    \"input_dim, output_dim, hidden_dim = 784, 47, 128\\n\",\r\n    \"\\n\",\r\n    \"weights_init = GlorotUniformInit(rng=rng)\\n\",\r\n    \"biases_init = ConstantInit(0.)\\n\",\r\n    \"\\n\",\r\n    \"# Create model with ONE hidden layer\\n\",\r\n    \"model = MultipleLayerModel([\\n\",\r\n    \"    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\\n\",\r\n    \"])\\n\",\r\n    \"\\n\",\r\n    \"error = CrossEntropySoftmaxError()\\n\",\r\n    \"# Use a Adam learning rule\\n\",\r\n    \"learning_rule = AdamLearningRule(learning_rate=learning_rate)\\n\",\r\n    \"\\n\",\r\n    \"# Remember to use notebook=False when you write a script to be run in a terminal\\n\",\r\n    \"_ = train_model_and_plot_stats(\\n\",\r\n    \"    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# Create model with TWO hidden layers\\n\",\r\n    \"model = MultipleLayerModel([\\n\",\r\n    \"    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\\n\",\r\n    \"    ReluLayer(),\\n\",\r\n    \"    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\\n\",\r\n    \"])\\n\",\r\n    \"\\n\",\r\n    \"error = CrossEntropySoftmaxError()\\n\",\r\n    \"# Use a Adam learning rule\\n\",\r\n    \"learning_rule = AdamLearningRule(learning_rate=learning_rate)\\n\",\r\n    \"\\n\",\r\n    \"# Remember to use notebook=False when you write a script to be run in a terminal\\n\",\r\n    \"_ = train_model_and_plot_stats(\\n\",\r\n    \"    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\"\r\n   ]\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.10.6\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 4\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/notebooks/Coursework_1.ipynb b/notebooks/Coursework_1.ipynb
--- a/notebooks/Coursework_1.ipynb	(revision f8f0f0736a6e611895d9f72d4355d9d2648427aa)
+++ b/notebooks/Coursework_1.ipynb	(date 1666362142300)
@@ -59,15 +59,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "KeysView(<numpy.lib.npyio.NpzFile object at 0x000002149A730280>)\n",
-      "KeysView(<numpy.lib.npyio.NpzFile object at 0x000002149A7303A0>)\n"
+      "KeysView(<numpy.lib.npyio.NpzFile object at 0x00000243C2D8A0A0>)\n",
+      "KeysView(<numpy.lib.npyio.NpzFile object at 0x00000243C2C53D90>)\n"
      ]
     }
    ],
@@ -78,6 +78,8 @@
     "# will probably not want to reload the data providers on\n",
     "# every training run. If you wish to reset their state you\n",
     "# should instead use the .reset() method of the data providers.\n",
+    "import sys\n",
+    "sys.path.append('C:/Users/admin/Desktop/mlpractical')\n",
     "import numpy as np\n",
     "import logging\n",
     "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
@@ -100,7 +102,586 @@
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "c1d9244b1f1a4d9aa57a091daf74b08d",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/100 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "6b59c0fa642a4f5c8c1ec9ab79a050eb",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 1: 7.0s to complete\n",
+      "    error(train)=9.30e-01, acc(train)=7.32e-01, error(valid)=9.51e-01, acc(valid)=7.28e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "b7e3a91fbb8e41fa838a09380b788385",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 2: 5.1s to complete\n",
+      "    error(train)=7.21e-01, acc(train)=7.86e-01, error(valid)=7.60e-01, acc(valid)=7.77e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "e58f4855b7654e9da07a59ef490121b4",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 3: 3.9s to complete\n",
+      "    error(train)=6.27e-01, acc(train)=8.06e-01, error(valid)=6.80e-01, acc(valid)=7.91e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "ca3b9636916b4c88aae6cac29f369077",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 4: 5.7s to complete\n",
+      "    error(train)=5.54e-01, acc(train)=8.27e-01, error(valid)=6.20e-01, acc(valid)=8.06e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "5b83d11d05c8481d9b1351d5280903b9",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 5: 4.4s to complete\n",
+      "    error(train)=5.22e-01, acc(train)=8.35e-01, error(valid)=5.97e-01, acc(valid)=8.10e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "6ae7da811cb84712bd28bfb9ae0a3751",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 6: 6.3s to complete\n",
+      "    error(train)=4.92e-01, acc(train)=8.40e-01, error(valid)=5.81e-01, acc(valid)=8.12e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "fa90faa5fc7145bfb119574f0b8b1a82",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 7: 7.1s to complete\n",
+      "    error(train)=4.66e-01, acc(train)=8.49e-01, error(valid)=5.68e-01, acc(valid)=8.18e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "aa38a7d9fd4842f2ae7b68e576e72d19",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 8: 4.7s to complete\n",
+      "    error(train)=4.37e-01, acc(train)=8.57e-01, error(valid)=5.44e-01, acc(valid)=8.25e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "9bef963af8f441c6b67591387549438e",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 9: 4.7s to complete\n",
+      "    error(train)=4.18e-01, acc(train)=8.62e-01, error(valid)=5.39e-01, acc(valid)=8.24e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "be31aa0173374497b33f7e952217e94c",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 10: 4.8s to complete\n",
+      "    error(train)=4.13e-01, acc(train)=8.62e-01, error(valid)=5.47e-01, acc(valid)=8.22e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "faab2af34166414b9b60783ba20b2ee2",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 11: 4.9s to complete\n",
+      "    error(train)=4.02e-01, acc(train)=8.64e-01, error(valid)=5.44e-01, acc(valid)=8.24e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "81231244907342968644967558166bcd",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 12: 4.7s to complete\n",
+      "    error(train)=3.88e-01, acc(train)=8.69e-01, error(valid)=5.37e-01, acc(valid)=8.29e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "16552641cc5a47c389b12529802d87a2",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 13: 4.5s to complete\n",
+      "    error(train)=3.78e-01, acc(train)=8.72e-01, error(valid)=5.36e-01, acc(valid)=8.29e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "72b4040f4bca4a1791e5fc9107cc342a",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 14: 4.0s to complete\n",
+      "    error(train)=3.66e-01, acc(train)=8.76e-01, error(valid)=5.41e-01, acc(valid)=8.27e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "2d30848915a14815971827d64d0ffc08",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 15: 4.2s to complete\n",
+      "    error(train)=3.60e-01, acc(train)=8.77e-01, error(valid)=5.37e-01, acc(valid)=8.29e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "df1cd092e1d8439091d793b5972a7683",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 16: 4.1s to complete\n",
+      "    error(train)=3.60e-01, acc(train)=8.74e-01, error(valid)=5.45e-01, acc(valid)=8.25e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "3a0e941bedc14e41bacd2356da1eea38",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 17: 4.1s to complete\n",
+      "    error(train)=3.40e-01, acc(train)=8.84e-01, error(valid)=5.34e-01, acc(valid)=8.32e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "e17bba4f8f7f4d4790541b77ac97dc8c",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 18: 4.0s to complete\n",
+      "    error(train)=3.44e-01, acc(train)=8.81e-01, error(valid)=5.50e-01, acc(valid)=8.25e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "caabf30ec284407e857af2c6ec619777",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 19: 4.1s to complete\n",
+      "    error(train)=3.34e-01, acc(train)=8.85e-01, error(valid)=5.43e-01, acc(valid)=8.31e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "bc8c4222db21468bb9d906e22072fe28",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 20: 4.3s to complete\n",
+      "    error(train)=3.25e-01, acc(train)=8.86e-01, error(valid)=5.43e-01, acc(valid)=8.27e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "930a312f05cc4a428b2d0c3a1a8a04d6",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 21: 4.6s to complete\n",
+      "    error(train)=3.28e-01, acc(train)=8.84e-01, error(valid)=5.54e-01, acc(valid)=8.22e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "e1ceccca616543e3ace3fd7ebc601008",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 22: 4.6s to complete\n",
+      "    error(train)=3.23e-01, acc(train)=8.86e-01, error(valid)=5.56e-01, acc(valid)=8.25e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "2c38fa7ca1e649838dc944bfc57e94ea",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 23: 4.8s to complete\n",
+      "    error(train)=3.10e-01, acc(train)=8.92e-01, error(valid)=5.53e-01, acc(valid)=8.27e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "39cb2aa499a241e7bec30896ccf7b5d2",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 24: 8.5s to complete\n",
+      "    error(train)=3.07e-01, acc(train)=8.92e-01, error(valid)=5.61e-01, acc(valid)=8.28e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "46ed8b140463446aa21e46b4970bf19f",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Epoch 25: 9.7s to complete\n",
+      "    error(train)=2.98e-01, acc(train)=8.96e-01, error(valid)=5.54e-01, acc(valid)=8.34e-01\n"
+     ]
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "d04e3c05eb9240c4bd6e6b56a845de64",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "  0%|          | 0/1000 [00:00<?, ?it/s]"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
    "source": [
     "# The model set up code below is provided as a starting point.\n",
     "# You will probably want to add further code cells for the\n",
@@ -179,7 +760,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.10.6"
+   "version": "3.9.12"
   }
  },
  "nbformat": 4,
diff --git a/test.py b/test.py
new file mode 100644
